# -*- coding: utf-8 -*-
"""Binary Classification of Amazon Product Ratings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BpTDbEilZ0UVvJ0KUBbIBlrx_DJATdxI

# Binary Classification of Amazon Product Ratings based on 1) Text only, 2) Sentiment Analysis only, and 3) Text + Sentiment Analysis

## Part 1: Binary Classification Based on Text only
"""

import pandas as pd
train_data = pd.read_csv('/content/train.csv')
test_data = pd.read_csv('/content/test.csv')

# preview train data
print(train_data.shape)
print(train_data.columns)
train_data.head()

# preview test data
print(test_data.shape)
print(test_data.columns)
test_data.head()

# creating the two labels for the 'overall' column in the training dataset
train_data['overall'] = train_data['overall'].apply(lambda f: 1 if f > 3 else 0)
train_data.head()

# text manipulation and analysis in both the training and testing datasets
train_text_columns = train_data['reviewText'].fillna('') + ' ' + train_data['summary'].fillna('')
test_text_columns = test_data['reviewText'].fillna('') + ' ' + test_data['summary'].fillna('')

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

# process the text data using tfidf
train_text_columns = vectorizer.fit_transform(train_text_columns.values.astype('U'))
test_text_columns = vectorizer.transform(test_text_columns.values.astype('U'))

from sklearn.model_selection import train_test_split
target = train_data['overall']

# split the data
x_train, x_test, y_train, y_test = train_test_split(train_text_columns, target, test_size=0.2, random_state=42)

# create a comprehensive function that covers the different evaluation metrics we'll examine in our models
from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score
def evaluate_model (y_actual, y_pred):
  confusion_matrix_metric = confusion_matrix(y_actual, y_pred)
  roc_auc_score_metric = roc_auc_score(y_actual, y_pred)
  accuracy_score_metric = accuracy_score(y_actual, y_pred)
  f1_score_metric = f1_score(y_actual, y_pred)

  print("Evaluation Metrics Scores for this model are")
  print("Confusion Matrix is\n", confusion_matrix_metric)
  print("ROC AUC Score is", roc_auc_score_metric)
  print("Accuray Score is", accuracy_score_metric)
  print("F1 Score is", f1_score_metric)

# classifier #1
# Logistic Regression
# this classifier does not filter stop words before processing data

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
import numpy as np
log_reg = LogisticRegression()

# prepare parameters
parameters = {'max_iter': [1000], 'C': [7.0]}
log_reg = GridSearchCV(log_reg, parameters, cv=5, scoring='f1_macro', n_jobs = -1)

# fit the model
log_reg.fit(train_text_columns, target)

# find the best parameters
best_log_reg_parameters = log_reg.best_params_

# build a model with the best parameters
best_log_reg = LogisticRegression(C=best_log_reg_parameters['C'], max_iter=best_log_reg_parameters['max_iter'])
best_log_reg.fit(x_train, y_train)

# evaluate the model based on the validation data set
y_pred = best_log_reg.predict(x_test)
evaluate_model(y_test, y_pred)

# make predictions on the test data set
log_reg_model_preds = best_log_reg.predict(test_text_columns)

# submit data
submission = pd.DataFrame({'id': test_data['id'], 'preds': log_reg_model_preds})
submission.to_csv('LogisticRegressionModel.csv', index=False)
submission.head()

# classifier #2
# SVCs
# this model filters stop words before processing data

# text manipulation and analysis in both the training and testing datasets
train_text_columns_preprocessed = train_data['reviewText'].fillna('') + ' ' + train_data['summary'].fillna('')
test_text_columns_preprocessed = test_data['reviewText'].fillna('') + ' ' + test_data['summary'].fillna('')

# preprocess the text data by cleaning it up from stop words
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

# create a set of stop words
stop_words = stopwords.words('english')

# clean up the training text data from stop words
train_text_columns_preprocessed = pd.DataFrame(train_text_columns_preprocessed)
train_text_columns_preprocessed.columns = ['text']
train_text_columns_preprocessed['text'] = train_text_columns_preprocessed['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
train_text_columns_preprocessed = train_text_columns_preprocessed['text'].fillna('')

# clean up the testing text data from stop words
test_text_columns_preprocessed = pd.DataFrame(test_text_columns_preprocessed)
test_text_columns_preprocessed.columns = ['text']
test_text_columns_preprocessed['text'] = test_text_columns_preprocessed['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))
test_text_columns_preprocessed = test_text_columns_preprocessed['text'].fillna('')

# process the text data using tfidf after cleaning it from stop words
train_text_columns_preprocessed = vectorizer.fit_transform(train_text_columns_preprocessed)
test_text_columns_preprocessed = vectorizer.transform(test_text_columns_preprocessed)

# split the data
x_train, x_test, y_train, y_test = train_test_split(train_text_columns_preprocessed, target, test_size=0.2, random_state=42)

# start building the SVC model process
from sklearn.svm import SVC
svc = SVC()
parameters = {'C': [7.0]}
svc_model = GridSearchCV(svc, parameters, cv=5, scoring='f1_macro', n_jobs = -1)

# fit model
svc_model.fit(x_train, y_train)
svc_model_best_parameters = svc_model.best_params_

# build a model with the best parameters
best_svc_model = SVC(C=svc_model_best_parameters['C'])
best_svc_model.fit(x_train, y_train)

# evaluate the model based on the validation data set
y_pred = best_svc_model.predict(x_test)
evaluate_model(y_test, y_pred)

# make predictions
svc_model_predictions = best_svc_model.predict(test_text_columns_preprocessed)

# submit data
submission = pd.DataFrame({'id': test_data['id'], 'preds': svc_model_predictions})
submission.to_csv('SVCModel.csv', index=False)
submission.head()

# classifier #3
# Perceptron
# this model does not filter stop words before processing data

from sklearn.linear_model import Perceptron
perceptron = Perceptron()
parameters = {'max_iter': [1000], 'penalty': ['l1']}
perceptron_model = GridSearchCV(perceptron, parameters, cv=5, scoring='f1_macro', n_jobs = -1)

# split the data
x_train, x_test, y_train, y_test = train_test_split(train_text_columns, target, test_size=0.2, random_state=42)

# fit model
perceptron_model.fit(x_train, y_train)
perceptron_model_best_parameters = perceptron_model.best_params_

# build a model with the best parameters
best_perceptron_model = Perceptron(max_iter=perceptron_model_best_parameters['max_iter'], penalty=perceptron_model_best_parameters['penalty'])
best_perceptron_model.fit(x_train, y_train)

# evaluate the model based on the validation data set
y_pred = best_perceptron_model.predict(x_test)
evaluate_model(y_test, y_pred)

# make predictions
perceptron_model_predictions = best_perceptron_model.predict(test_text_columns)

# submit data
submission = pd.DataFrame({'id': test_data['id'], 'preds': perceptron_model_predictions})
submission.to_csv('PerceptronModel.csv', index=False)
submission.head()

# classifier #4
# Random Forest
# this model does not filter stop words before processing data

from sklearn.ensemble import RandomForestClassifier
random_forest = RandomForestClassifier()
parameters = {'max_depth': [10]}
random_forest_model = GridSearchCV(random_forest, parameters, cv=5, scoring='f1_macro', n_jobs = -1)

# fit model
random_forest_model.fit(x_train, y_train)
best_random_forest_parameters = random_forest_model.best_params_

# build a model with the best parameters
best_random_forst_model = RandomForestClassifier(max_depth=best_random_forest_parameters['max_depth'])
best_random_forst_model.fit(x_train, y_train)

# evaluate the model based on the validation data set
y_pred = best_random_forst_model.predict(x_test)
evaluate_model(y_test, y_pred)

# make predictions
random_forest_model_predictions = best_random_forst_model.predict(test_text_columns)

# submit data
submission = pd.DataFrame({'id': test_data['id'], 'preds': random_forest_model_predictions})
submission.to_csv('RandomForestModel.csv', index=False)
submission.head()

# best model
# after running several rounds of the four classificatin model,
# the logistic regression model proved to have the best F1 macro score
# let's see what the best hyperparameters were for the best logistic regression model we found
print(best_log_reg_parameters)

"""##Part 2: Predictions Based on Sentiment Analysis only"""

import pandas as pd
train_data = pd.read_csv('/content/train.csv')
test_data = pd.read_csv('/content/test.csv')

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# 
# !pip install vaderSentiment
# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

# Initalize the sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

import pandas as pd

test = pd.read_csv("/content/test.csv")
train = pd.read_csv("/content/train.csv")
train['overall'] = train_data['overall'].apply(lambda f: 1 if f > 3 else 0)

print(train.shape)
print(test.shape)

print(train.columns)
print(test.columns)
train.head()

from re import X
from sklearn.ensemble import RandomForestClassifier

features = ["reviewerID", "reviewText", "verified"]

X = train[features]
y = train['reviewText']
X

from pprint import pprint
positive_text = "my grandson loved this. it is a great toy"
neutral_text = "wrong part. our fault"
negative_text = "this wire set it really sucks!!!"

for text in [positive_text, neutral_text, negative_text]:
  sentiment_scores = analyzer.polarity_scores(text)
  pprint(text)
  pprint(sentiment_scores)
  print("*"*60)

import numpy as np
from sklearn.model_selection import train_test_split
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

train_reviews = train['reviewText'].fillna('') + ' ' + train['summary'].fillna('')
test_reviews = test['reviewText'].fillna('') + '' + test['summary'].fillna('')

def analyzing_sen(sentimentAnalysisFunc, data_series):
  return data_series.apply(sentimentAnalysisFunc.polarity_scores).apply(lambda x: 0 if x['neg'] > x['pos'] else 1)
train_reviews_encoded = analyzing_sen(analyzer, train_reviews)
test_reviews_encoded = analyzing_sen(analyzer, test_reviews)

print(train_reviews_encoded)
print(test_reviews_encoded)

print(train['overall'].shape)

from sklearn.linear_model import LogisticRegression, Perceptron
from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score

#These were the best parameters found from part 1
#log_reg = LogisticRegression(max_iter=[1000], C=7.0)
# log_reg = LogisticRegression()
x_train, x_test, y_train, y_test = train_test_split(train_reviews_encoded, train['overall'], test_size=0.2, random_state=42)

# fit model
#best_log_reg = LogisticRegression(C=7.0, max_iter= 1000)
best_log_reg.fit(x_train.values.reshape(-1,1), y_train.values.reshape(-1,1))

# evaluate the model based on the validation data set
y_pred = best_log_reg.predict(x_test.array.reshape(-1, 1))

def evaluate_model (y_actual, y_pred):
  confusion_matrix_metric = confusion_matrix(y_actual, y_pred)
  roc_auc_score_metric = roc_auc_score(y_actual, y_pred)
  accuracy_score_metric = accuracy_score(y_actual, y_pred)
  f1_score_metric = f1_score(y_actual, y_pred)

  print("Evaluation Metrics Scores for this model are")
  print("Confusion Matrix is\n", confusion_matrix_metric)
  print("ROC AUC Score is", roc_auc_score_metric)
  print("Accuray Score is", accuracy_score_metric)
  print("F1 Score is", f1_score_metric)

evaluate_model(y_test, y_pred)

# make predictions
logreg_preds = best_log_reg.predict(train_reviews_encoded.values.reshape(-1, 1))

threshold = 4
logreg_pred_binary = np.where(logreg_preds >= threshold, 1, 0)

set(logreg_pred_binary)

#sumbit data
submission = pd.DataFrame({'id': train_data['id'], 'preds': logreg_preds})
submission.to_csv('Logregprediction2.csv', index=False)
submission.head()

def analyzing_sen(sentimentAnalysisFunc, data_series):
  return data_series.apply(sentimentAnalysisFunc.polarity_scores).apply(lambda x: 0 if x['neg'] > x['pos'] else 1)
train_reviews_encoded = analyzing_sen(analyzer, train_reviews)
test_reviews_encoded = analyzing_sen(analyzer, test_reviews)

"""##Part 3: Predictions Based on Text + Sentiment Analysis"""

from scipy.sparse import hstack
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score
from sklearn.linear_model import LogisticRegression

# Initialize the sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Analyze sentiment for the training and test data
train_reviews_encoded = train_reviews.apply(lambda text: 1 if analyzer.polarity_scores(text)['compound'] >= 0 else 0)
test_reviews_encoded = test_reviews.apply(lambda text: 1 if analyzer.polarity_scores(text)['compound'] >= 0 else 0)

# Combine text and sentiment data for test data
analyzer_test = test_reviews_encoded.values.reshape(-1, 1)
analyzer_train = train_reviews_encoded.values.reshape(-1, 1)

train_combined = hstack([train_text_columns, analyzer_train])
test_combined = hstack([test_text_columns, analyzer_test])

# Split the data into train and test
x_train, x_test, y_train, y_test = train_test_split(train_combined, train['overall'], test_size=0.2, random_state=42)

# Initialize and train the logistic regression model with appropriate hyperparameters
best_log_reg = LogisticRegression(C=7.0, max_iter=1000)
best_log_reg.fit(x_train, y_train)

# Make predictions on the validation data set
y_pred = best_log_reg.predict(x_test)

# evaluate model
evaluate_model(y_pred, y_test)

# Make predictions on the test data set
log_reg_model_preds = best_log_reg.predict(test_combined)

# Submit data
submission = pd.DataFrame({'id': test_data['id'], 'pred': log_reg_model_preds})
submission.to_csv('LogisticRegressionModelSentimentAndText2.csv', index=False)
submission.head()